"""
Monadical-ML_Take_Home.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AHxICLQ35OSpk-HKVIAcezG2DiaMNFk7
"""

from google.colab import drive
from datasets import load_dataset
from transformers import AutoFeatureExtractor, AutoModel
from torchvision.transforms import Compose, Normalize, RandomHorizontalFlip,\
    RandomResizedCrop, Resize, \
    ToTensor, RandomVerticalFlip, RandomRotation
import torch

from torch import nn
from transformers import ViTConfig

from tqdm.notebook import tqdm
from torch.optim import AdamW
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader

drive.mount('/content/drive')

BASE_DIR = "/content/drive/MyDrive/Monadical experiments"
MODEL_NAME = BASE_DIR + "/mlmodels/vit-potatoes-plant-health-status/"
DATASET_FOLDER = BASE_DIR + "/dataset/PLD_3_Classes_256"
TRAIN = "/Training"
VALIDATION = "/Validation"
TEST = "/Testing"
TRAIN_FOLDER = DATASET_FOLDER + TRAIN
TEST_FOLDER = DATASET_FOLDER + TEST
VALIDATION_FOLDER = DATASET_FOLDER + VALIDATION

train_dataset = load_dataset(path=TRAIN_FOLDER)
test_dataset = load_dataset(path=TEST_FOLDER)
val_dataset = load_dataset(path=VALIDATION_FOLDER)

# train_dataset["test"] = test_dataset["train"]

ex = train_dataset["train"][1]
# display(ex["image"])
# print(ex["label"])
# print(train_dataset["train"])

# print(train_dataset["train"].features["label"])
# print(test_dataset["train"].features["label"])
# print(val_dataset["train"].features["label"])


feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME, add_pooling_layer=False)
# print(feature_extractor)
# print(model)


mean = feature_extractor.image_mean
std = feature_extractor.image_std
# size = feature_extractor.size
SIZE = 224


class PotatoLeafDataset(torch.utils.data.Dataset):

    def __init__(self, input_data, intent="train"):
        self.input_data = input_data
        self.intent = intent
        self.train_transforms = Compose(
            [
                RandomResizedCrop(SIZE),
                RandomHorizontalFlip(p=0.5),
                RandomVerticalFlip(p=0.5),
                RandomRotation(40),
                ToTensor(),
                Normalize(mean=mean, std=std)
            ])
        self.test_val_transforms = Compose(
            [
                Resize(SIZE),
                ToTensor(),
                Normalize(mean=mean, std=std)
            ])

    def __len__(self):
        return len(self.input_data)

    def get_image_data(self, idx):
        if self.intent == "train":
            return self.train_transforms(
                self.input_data[idx]['image'].convert("RGB"))
        return self.test_val_transforms(
            self.input_data[idx]['image'].convert("RGB"))

    def get_label_data(self, idx):
        return self.input_data[idx]['label']

    def __getitem__(self, idx):
        image_data = self.get_image_data(idx)
        label_data = self.get_label_data(idx)
        return image_data, label_data


vit_config = ViTConfig()


# print(vit_config)

class FineTunedVITModel(nn.Module):
    def __init__(self, config=vit_config, num_labels=3):
        super(FineTunedVITModel, self).__init__()

        self.finetunedmodel = AutoModel.from_pretrained(MODEL_NAME)
        self.custom_classifier = (
            nn.Linear(vit_config.hidden_size, num_labels)
        )

    def forward(self, x):
        x = self.finetunedmodel(x)['last_hidden_state']
        output = self.custom_classifier(x[:, 0, :])
        return output


BATCH_SIZE = 12
LEARNING_RATE = 5e-5
EPOCHS = 5
TRAIN = PotatoLeafDataset(train_dataset["train"], "train")
TRAIN_DATALOADER = DataLoader(TRAIN, batch_size=BATCH_SIZE, shuffle=True)
VAL = PotatoLeafDataset(val_dataset["train"], "val")
VAL_DATALOADER = DataLoader(VAL, batch_size=BATCH_SIZE, shuffle=True)


def finetune_training():
    cuda_available = torch.cuda.is_available()
    device = torch.device("cuda" if cuda_available else "cpu")

    model = FineTunedVITModel().to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE,
                      betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)
    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0,
                                      end_factor=0.5, total_iters=10)

    for i in range(EPOCHS):
        accuracy = 0.0
        total_training_loss = 0.0
        total_val_loss = 0.0
        val_accuracy = 0.0

        for sample_image, sample_label in tqdm(TRAIN_DATALOADER):
            model.train()
            output = model(sample_image.to(device))
            loss = criterion(output, sample_label.to(device))
            acc = (output.argmax(dim=1) ==
                   sample_label.to(device)).sum().item()
            accuracy += acc
            total_training_loss += loss.item()

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        print(
            f'Epochs: {i + 1} |'
            f' Loss: {total_training_loss / len(TRAIN_DATALOADER): .3f}'
            f' | Accuracy: {accuracy / len(TRAIN): .3f}')

        scheduler.step()

        for sample_image, sample_label in tqdm(VAL_DATALOADER):
            model.eval()
            with torch.no_grad():
                output = model(sample_image.to(device))
            loss = criterion(output, sample_label.to(device))
            acc = (output.argmax(dim=1)
                   == sample_label.to(device)).sum().item()
            val_accuracy += acc
            total_val_loss += loss.item()
        print(
            f'Epochs: {i + 1} |'
            f' Loss: {total_val_loss / len(VAL_DATALOADER): .3f} |'
            f' Accuracy: {val_accuracy / len(VAL): .3f}')

    return model

# model = None
# finetuned_model = None
# import gc
# gc.collect()


finetuned_model = finetune_training()

# EXPERIMENTATIONS RESULTS ON MODEL FINETUNING
# BS 8,LR 1e-4, EPOCHS 3

# 100%|██████████| 407/407 [25:23<00:00,  3.74s/it]
# Epochs: 1 | Loss:  0.108 | Accuracy:  0.978
# 100%|██████████| 407/407 [01:57<00:00,  3.45it/s]
# Epochs: 2 | Loss:  0.148 | Accuracy:  0.969
# 100%|██████████| 407/407 [01:59<00:00,  3.40it/s]
# Epochs: 3 | Loss:  0.148 | Accuracy:  0.966

# BS 12, LR 1e-4, EPOCHS 10
# 163/163 [20:06<00:00, 9.98s/it]
# Epochs: 1 | Loss:  0.268 | Accuracy:  0.927
# 21/21 [04:03<00:00, 10.89s/it]
# Epochs: 1 | Loss:  2.944 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.61it/s]
# Epochs: 2 | Loss:  0.291 | Accuracy:  0.930
# 21/21 [00:07<00:00, 3.09it/s]
# Epochs: 2 | Loss:  2.861 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.59it/s]
# Epochs: 3 | Loss:  0.265 | Accuracy:  0.935
# 21/21 [00:07<00:00, 2.80it/s]
# Epochs: 3 | Loss:  2.855 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.59it/s]
# Epochs: 4 | Loss:  0.287 | Accuracy:  0.929
# 21/21 [00:07<00:00, 2.86it/s]
# Epochs: 4 | Loss:  2.849 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.59it/s]
# Epochs: 5 | Loss:  0.296 | Accuracy:  0.918
# 21/21 [00:07<00:00, 3.10it/s]
# Epochs: 5 | Loss:  2.849 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.60it/s]
# Epochs: 6 | Loss:  0.329 | Accuracy:  0.913
# 21/21 [00:08<00:00, 2.67it/s]
# Epochs: 6 | Loss:  2.744 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.60it/s]
# Epochs: 7 | Loss:  0.283 | Accuracy:  0.923
# 21/21 [00:07<00:00, 2.85it/s]
# Epochs: 7 | Loss:  2.821 | Accuracy:  0.363
# 163/163 [01:46<00:00, 1.60it/s]
# Epochs: 8 | Loss:  0.222 | Accuracy:  0.949
# 21/21 [00:07<00:00, 3.12it/s]
# Epochs: 8 | Loss:  2.863 | Accuracy:  0.363


# BS 12, LR 5e-4, EPOCHS 10
# scheduler = lr_scheduler.LinearLR(optimizer,
# start_factor=1.0, end_factor=0.5, total_iters=30)
# 271/271 [01:51<00:00, 2.45it/s]
# Epochs: 1 | Loss:  0.119 | Accuracy:  0.974
# 35/35 [00:07<00:00, 5.33it/s]
# Epochs: 1 | Loss:  4.149 | Accuracy:  0.363
# 271/271 [01:51<00:00, 2.48it/s]
# Epochs: 2 | Loss:  0.201 | Accuracy:  0.956
# 35/35 [00:07<00:00, 5.33it/s]
# Epochs: 2 | Loss:  3.709 | Accuracy:  0.363
# 271/271 [01:52<00:00, 2.44it/s]
# Epochs: 3 | Loss:  0.384 | Accuracy:  0.912
# 35/35 [00:07<00:00, 5.28it/s]
# Epochs: 3 | Loss:  4.017 | Accuracy:  0.363
# 271/271 [01:52<00:00, 2.43it/s]
# Epochs: 4 | Loss:  0.453 | Accuracy:  0.893
# 35/35 [00:07<00:00, 4.72it/s]
# Epochs: 4 | Loss:  3.378 | Accuracy:  0.363
# 271/271 [01:51<00:00, 2.46it/s]
# Epochs: 5 | Loss:  0.473 | Accuracy:  0.886
# 35/35 [00:07<00:00, 4.90it/s]
# Epochs: 5 | Loss:  3.122 | Accuracy:  0.363
# 271/271 [01:51<00:00, 2.46it/s]
# Epochs: 6 | Loss:  0.587 | Accuracy:  0.838
# 35/35 [00:08<00:00, 5.08it/s]
# Epochs: 6 | Loss:  2.949 | Accuracy:  0.363
# 271/271 [01:52<00:00, 2.44it/s]
# Epochs: 7 | Loss:  0.603 | Accuracy:  0.843
# 35/35 [00:08<00:00, 5.18it/s]
# Epochs: 7 | Loss:  2.735 | Accuracy:  0.363
# 271/271 [01:52<00:00, 2.42it/s]
# Epochs: 8 | Loss:  0.450 | Accuracy:  0.878
# 35/35 [00:07<00:00, 5.28it/s]
# Epochs: 8 | Loss:  2.859 | Accuracy:  0.363
# 271/271 [01:52<00:00, 2.43it/s]
# Epochs: 9 | Loss:  0.288 | Accuracy:  0.929
# 35/35 [00:07<00:00, 5.25it/s]
# Epochs: 9 | Loss:  2.936 | Accuracy:  0.363
# 271/271 [01:52<00:00, 2.45it/s]
# Epochs: 10 | Loss:  0.421 | Accuracy:  0.882
# 35/35 [00:07<00:00, 5.26it/s]
# Epochs: 10 | Loss:  2.978 | Accuracy:  0.363

# Convert image to RGB
# 271/271 [01:52<00:00, 2.43it/s]
# Epochs: 7 | Loss:  0.126 | Accuracy:  0.974
# 35/35 [00:07<00:00, 4.81it/s]
# Epochs: 7 | Loss:  3.036 | Accuracy:  0.363

# Final
# 271/271 [08:00<00:00, 1.64s/it]
# Epochs: 1 | Loss:  0.274 | Accuracy:  0.908
# 35/35 [01:39<00:00, 2.57s/it]
# Epochs: 1 | Loss:  0.050 | Accuracy:  0.998
# 271/271 [01:55<00:00, 2.36it/s]
# Epochs: 2 | Loss:  0.128 | Accuracy:  0.954
# 35/35 [00:08<00:00, 5.00it/s]
# Epochs: 2 | Loss:  0.071 | Accuracy:  0.981
# 271/271 [01:54<00:00, 2.39it/s]
# Epochs: 3 | Loss:  0.124 | Accuracy:  0.958
# 35/35 [00:08<00:00, 5.02it/s]
# Epochs: 3 | Loss:  0.024 | Accuracy:  0.995
# 271/271 [01:54<00:00, 2.41it/s]
# Epochs: 4 | Loss:  0.117 | Accuracy:  0.955
# 35/35 [00:07<00:00, 5.08it/s]
# Epochs: 4 | Loss:  0.018 | Accuracy:  0.998
# 271/271 [01:54<00:00, 2.40it/s]
# Epochs: 5 | Loss:  0.110 | Accuracy:  0.962
# 35/35 [00:08<00:00, 4.68it/s]
# Epochs: 5 | Loss:  0.023 | Accuracy:  0.995

# finetuned_model

# SAVE MODELS
model_save_name = 'finetuned_model_2.pt'
path = F"/content/drive/My Drive/Monadical experiments/" \
       F"mlmodels/{model_save_name}"
torch.save(finetuned_model.state_dict(), path)

model_save_name = 'finetuned_model_2.bin'
path = F"/content/drive/My Drive/Monadical experiments/" \
       F"mlmodels/{model_save_name}"
torch.save(finetuned_model, path)

model_save_name = 'finetuned_model_2_cpu.pt'
path = F"/content/drive/My Drive/Monadical experiments/" \
       F"mlmodels/{model_save_name}"
torch.save(finetuned_model.to("cpu").state_dict(), path)

model_save_name = 'finetuned_model_2_cpu.bin'
path = F"/content/drive/My Drive/Monadical experiments/" \
       F"mlmodels/{model_save_name}"
torch.save(finetuned_model.to("cpu"), path)

TEST = PotatoLeafDataset(test_dataset["train"], "test")
TEST_DATALOADER = DataLoader(TEST, batch_size=BATCH_SIZE)

# MODEL FINAL ACCURACY ON TEST DATA

cuda_available = torch.cuda.is_available()
device = torch.device("cuda" if cuda_available else "cpu")
criterion = nn.CrossEntropyLoss().to(device)
test_accuracy = 0.0
total_test_loss = 0.0

for sample_image, sample_label in tqdm(TEST_DATALOADER):
    finetuned_model.eval()
    with torch.no_grad():
        finetuned_model.to(device)
        output = finetuned_model(sample_image.to(device))
    loss = criterion(output, sample_label.to(device))
    acc = (output.argmax(dim=1) == sample_label.to(device)).sum().item()
    test_accuracy += acc
    total_test_loss += loss.item()

print(f'Loss: {total_test_loss / len(TEST_DATALOADER): .3f} |'
      f' Test accuracy: {test_accuracy / len(TEST): .3f}')
